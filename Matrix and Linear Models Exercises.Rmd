---
title: "Matrix and Linear Models Exercises"
author: "Emre Usenmez - github.com/emre-us"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# HarvardX PH525.2x: Intro to Linear Models and Matrix Algebra - Exercises

## Intro Exercises

Install the library UsingR:

```{r}
install.packages("UsingR")
```

Access Galton's father and son heights:

```{r}
library(UsingR)
data("father.son", package = "UsingR")
```

Q1. What is the average height of the sons?

```{r}
summary(father.son)
```

One of the defining features of regression is that we stratify one variable based on others. In Statistics we use the verb "condition". For example, the linear model for son and father heights answers the question how tall do I expect a son to be if I condition on his father being x inches. The regression line answers this question for any x.

Using Galton's dataset, we want to know the expected height of sons if we condition on the father being 71 inches. Create a list of son heights for sons that have fathers with heights of 71 inches (round to the nearest inch). What is the mean of the son heights for fathers that have a height of 71 inches?

```{r}
library(dplyr)
fh71 <- father.son %>%
  filter(round(fheight) == 71)

mean(fh71$sheight)

# Alternatively:
mean(father.son$sheight[round(father.son$fheight)==71])
```










## Matrix Notation Exercises

In R we have vectors and matrices. You can create your own vectors with the function c(). Vectors are also the output of many functions such as:

```{r}
rnorm(5)
```

You can turn vectors into matrices using functions such as rbind(), cbind(), or matrix().

```{r}
X = matrix(1:1000, 100, 10)
```

Q1. What is the entry in row 25, column 3?

```{r}
X[25,3]
```

Q2. Using the function cbind(), create a 10x5 matrix with first column x=1:10. Then use 2*x, 3*x, 4*x, and 5*x, respectively in columns 2 through 5. What is the sum of the elements of the 7th row?

```{r}
x1 <- 1:10

X <- cbind(x1, x*2, x*3, x*4, x*5)
dim(X)

sum(X[7,])
```

Q3. Create a matrix with multiples in the 3rd column.

```{r}
X <- matrix(1:60,20,3, byrow = TRUE)

#to check if 3rd column has all multiples of 3:
all(X[,3]%%3==0) #The symbol "%%" refer to an arithmetic operation called modulo operation. It gives the remainder after x divided by y. So here if all the 3rd column is divided by 3, the remainder would need to be 0 for the entire column to be multiples of 3. This is a quick way of checking it.

```

Q5. What is the last element of the vector returned by seq(10, 1, -2)?

```{r}
seq(10,1,-2)
```










## Notes on Inverse Matrix

The inverse of matrix X, denoted with $X^-1$ has the property that, when multiplied, gives you the identity $X^{-1}X=I$. 

This is an identity matrix:
```{r}
diag(4)
```


Of course, not all matrices have inverses. For example, a $2\times 2$ matrix with 1s in all its entries does not have an inverse. 

As we will see when we get to the section on applications to linear models, being able to compute the inverse of a matrix is quite useful. A very convenient aspect of R is that it includes a predefined function `solve` to do this. Here is how we would use it to solve the linear of equations:

$$
\begin{align*}
a + b + c &= 6\\
3a - 2b + c &= 2\\
2a + b  - c &= 1
\end{align*}
$$
We described how this system can be rewritten and solved using matrix algebra:

$$
\,
\begin{pmatrix}
1&1&1\\
3&-2&1\\
2&1&-1
\end{pmatrix}
\begin{pmatrix}
a\\
b\\
c
\end{pmatrix} =
\begin{pmatrix}
6\\
2\\
1
\end{pmatrix}
\implies
\begin{pmatrix}
a\\
b\\
c
\end{pmatrix}=
\begin{pmatrix}
1&1&1\\
3&-2&1\\
2&1&-1
\end{pmatrix}^{-1}
\begin{pmatrix}
6\\
2\\
1
\end{pmatrix}
$$
Here is how we would use it to solve the linear of equations in R:

```{r}
X <- matrix(c(1,3,2,1,-2,1,1,1,-1),
            3,3)
y <- matrix(c(6,2,1), 
            3, 1)

solve(X)%*%y #equivalent to solve (X,y). "%*% is to do a matrix multiplication. Note that "solve()" is a function that should be used with caution as it is not generally numerically stable.
```










## Matrix Operation Exercises

Q2. Solve for the following system of equations:

$$
\begin{align*}
3a + 4b + 5c + d &= 10\\
2a + 2b + 2c - d &= 5\\
a - b  + 5c - 5d &= 7\\
5a + d &= 4\\
\end{align*}
$$
```{r}
X <- matrix(c(3, 2, 1, 5, 4, 2, -1, 0, -5, 2, 5, 0, 1, -1, -5, 1), 
            4, 4)
y <- matrix(c(10, 5, 7, 4))

solve(X,y)
```


Load the following two matrices into R:

```{r}
a <- matrix(1:12, nrow = 4)
b <- matrix(1:15, nrow = 3)
```

In the questions below we will use the matrix multiplication operator in R, $ %*% $, to multiply these two matrices.

Q3. What is the value in the 3rd row and 2nd column of the matrix product of a and b?

```{r}
a%*%b
```

Q4. Multiply the 3rd row of a with the second column of b using the element-wise vector multiplication with *. What is the sum of the elements in the resulting vector?

```{r}
a[3,] %*% b[,2]

# alternatively

sum(a[3,]*b[,2])
```

This is equivalent to the 3rd row, 2nd column element of the product of the two matrices.










## Matrix Algebra Examples

We start with some simple examples and eventually arrive at the main one: how to write linear models with matrix algebra notation and solve the least squares problem.





### The Average or Mean

To compute the sample average and variance of our data, we use these formulas $\bar{Y}=\frac{1}{N} Y_i$ and $\mbox{var}(Y)=\frac{1}{N} \sum_{i=1}^N (Y_i - \bar{Y})^2$. We can represent these with matrix multiplication. First, define this $N \times 1$ matrix made just of 1s:

$$
A=\begin{pmatrix}
1\\
1\\
\vdots\\
1
\end{pmatrix}
$$

This implies that:

$$
\frac{1}{N}
\mathbf{A}^\top Y = \frac{1}{N}
\begin{pmatrix}1&1&\dots&1\end{pmatrix}
\begin{pmatrix}
Y_1\\
Y_2\\
\vdots\\
Y_N
\end{pmatrix}=
\frac{1}{N} \sum_{i=1}^N Y_i
= \bar{Y}
$$

Note that we are multiplying by the scalar $1/N$. In R, we multiply matrix using `%*%`:

```{r}
mean(father.son$sheight)

#In Matrix Algebra:
N <- length(father.son$sheight)
Y <- matrix(data = father.son$sheight,
            nrow = N,
            ncol = 1)
A <- matrix(data = 1,
            nrow = N,
            ncol = 1)
barY = (t(A) %*% Y) / N

barY
```





### The Variance

Mltiplying the transpose of a matrix with another is very common in statistics. In fact, it is so common that there is a function in R:

```{r}
barY = crossprod(A,Y) / N
barY
```

For the variance, we note that if:

$$
\mathbf{r}\equiv \begin{pmatrix}
Y_1 - \bar{Y}\\
\vdots\\
Y_N - \bar{Y}
\end{pmatrix}, \,\,
\frac{1}{N} \mathbf{r}^\top\mathbf{r} = 
\frac{1}{N}\sum_{i=1}^N (Y_i - \bar{Y})^2
$$

In R, if you only send one matrix into `crossprod`, it computes: $r^\top r$ so we can simply type:

```{r}
r <- y - barY
crossprod(r) / N
```





### Linear Models

Now we are ready to put all this to use. Let's start with Galton's example. If we define these matrices:
 
$$
\mathbf{Y} = \begin{pmatrix}
Y_1\\
Y_2\\
\vdots\\
Y_N
\end{pmatrix}
,
\mathbf{X} = \begin{pmatrix}
1&x_1\\
1&x_2\\
\vdots\\
1&x_N
\end{pmatrix}
,
\mathbf{\beta} = \begin{pmatrix}
\beta_0\\
\beta_1
\end{pmatrix} \mbox{ and }
\mathbf{\varepsilon} = \begin{pmatrix}
\varepsilon_1\\
\varepsilon_2\\
\vdots\\
\varepsilon_N
\end{pmatrix}
$$



Then we can write the model:

$$ 
Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i, i=1,\dots,N 
$$


as: 


$$
\,
\begin{pmatrix}
Y_1\\
Y_2\\
\vdots\\
Y_N
\end{pmatrix} = 

\begin{pmatrix}
1&x_1\\
1&x_2\\
\vdots\\
1&x_N
\end{pmatrix}

\begin{pmatrix}
\beta_0\\
\beta_1
\end{pmatrix} +

\begin{pmatrix}
\varepsilon_1\\
\varepsilon_2\\
\vdots\\
\varepsilon_N
\end{pmatrix}

$$

or simply: 

$$
\mathbf{Y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon}
$$

which is a much simpler way to write it.

One of the ways in which this is useful is that we can write down the residual sum of squares in a relatively simple formula. One of the tasks we needed to perform was to minimise the residual sums of squares. Now that we have this in matrix notaton, the least squares equation becomes simpler as well since it is the following cross-product:

$$
(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})^\top
(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})
$$
This is the sum of squares of the error terms. So now we are ready to determine which values of $\beta$ minimize the above, which we  can do  using calculus to find the minimum. 



#### Advanced: Finding the minimum using calculus

There are a series of rules that permit us to compute partial derivative equations in matrix notation. By equating the derivative to 0 and solving for the $\beta$, we will have our solution. The only one we need here tells us that the derivative of the above equation is:

$$
2 \mathbf{X}^\top (\mathbf{Y} - \mathbf{X} \boldsymbol{\hat{\beta}})=0
$$

By rearranging:

$$
\mathbf{X}^\top \mathbf{X} \boldsymbol{\hat{\beta}} = \mathbf{X}^\top \mathbf{Y}   
$$
we see that $\hat{\beta}$ is:

$$
\boldsymbol{\hat{\beta}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{Y}   
$$

and we have our solution. We usually put a hat on the $\beta$ that solves this, $\hat{\beta}$ , as it is an estimate of the "real" $\beta$ that generated the data.

Remember that the least squares are like a square (multiply something by itself) and that this formula is similar to the derivative of $f(x)^2$ being $2f(x)f\prime (x)$. 

This format for mimimising sum of residual squares is one of the most widely used results in data analysis. This is because it gives us estimates of the unknown parameters for any linear model. 

This is pretty simple to do in R.




### Finding LSE in R

To find the least squares estimate in R:

```{r}
X <- cbind(1,father.son$fheight) #this binds first column of 1s and fathers' heights as second column
Y <- father.son$sheight

betahat <- solve(t(X) %*% X) %*% t(X) %*% Y
betahat
#or

betahat <- solve(crossprod(X)) %*% crossprod(X,Y)
betahat
```

We can look at how this fits with the data by computing the the estimated $\hat{\beta}_0+\hat{\beta}_1 x$ for any value of $x$:

```{r}
newx <- seq(min(father.son$fheight),
            max(father.son$fheight),
            length = 100)

X <- cbind(1, newx)

fitted <- X %*% betahat

plot(x = father.son$fheight,
     y = father.son$sheight,
     xlab = "Father's height",
     ylab = "Son's height")

lines(x = newx, y = fitted, col=2)
```

It fits data very well. In this case $\hat\beta_1$ is 0.514093 which represents the influence that the father has on the sons' heights.

This $\hat{\boldsymbol{\beta}}=(\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{Y}$ is one of the most widely used results in data analysis. One of the advantages of this approach is that we can use it in many different situations.  For example, in our falling object problem: 
 
```{r}
set.seed(1)
g <- 9.8 #meters per second
n <- 25
tt <- seq(0,3.4, len = n) #time in seconds, t is a base function
d <- 56.67 - 0.5*g*tt^2 + rnorm(n, sd=1)
```

We will use almost exactly the same code as the one we did for the father-son heights relationship:

```{r}
X <- cbind(1, tt, tt^2)
Y <- d

betahat <- solve(crossprod(X)) %*% crossprod(X,Y)

newtt <- seq(min(tt),
             max(tt),
             len = 100)

X <- cbind(1, newtt, newtt^2)

fitted <- X %*% betahat

plot(tt, Y, xlab = "Time", ylab = "Height")
lines(newtt, fitted, col=2)
```

We can look at the resuling estimates as well:

```{r}
betahat
```

56.5317368 is roughly the height of Tower of Pisa which is 56 meters high. Since we are dropping the object from a stationary position so the initial velocity should be 0. We get 0.5013565 which is very close to 0. The third term, the one that goes in from of $tt^2$ is about half of 9.8 which is the constant of acceleration, roughly 4.9 meters per second squared. We get -0.50386455 which is pretty close. It is negative because the object is falling.
